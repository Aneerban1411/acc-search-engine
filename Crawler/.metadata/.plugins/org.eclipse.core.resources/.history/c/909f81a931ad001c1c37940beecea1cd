package webcrawler;

import org.jsoup.Jsoup;
import org.jsoup.Jsoup;   
import org.jsoup.nodes.Document;   
import org.jsoup.nodes.Element;   
import org.jsoup.select.Elements; 
import java.io.IOException;
import java.util.HashMap; 

public class Crawler {

	private static HashMap<String, String> urlLinks = new HashMap<String,String>();
	private static final int maximum_depth = 2;
	
	public Crawler()
	{
		
		//maximum_depth = 3;
	}
	public static void crawl(String url, int depth)
	{
		if(urlLinks.size() < 50 && !urlLinks.containsKey(url) && depth < maximum_depth)
		{
			try {
				Document page = Jsoup.connect(url).get();
				urlLinks.put(url, page.title());
				depth++;
				Elements linksOnUrl = page.select("a[href]");
				
				for(Element link : linksOnUrl)
				{
					crawl(link.attr("abs:href"), depth);
				}
				
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
		}
	}
	public static void main(String[] args) {
		crawl("https://www.javatpoint.com/digital-electronics", 0);
		for(String key : urlLinks.keySet())
		{
			System.out.println(urlLinks.get(key) + " Title: ");
		}
			
	}

}
