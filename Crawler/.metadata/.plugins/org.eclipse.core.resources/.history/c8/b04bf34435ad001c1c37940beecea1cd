package webcrawler;

import org.jsoup.Jsoup;
import org.jsoup.Jsoup;   
import org.jsoup.nodes.Document;   
import org.jsoup.nodes.Element;   
import org.jsoup.select.Elements; 
import java.io.IOException;
import java.util.HashMap; 

public class Crawler {

	private static HashMap<String, PageData> urlLinks = new HashMap<String, PageData>();
	private static final int maximum_depth = 2;
	
	
	public static void crawl(String url, int depth)
	{
		if(urlLinks.size() < 50 && !urlLinks.containsKey(url) && depth < maximum_depth)
		{
			try {
				Document page = Jsoup.connect(url).get();
				
				urlLinks.put(url, new PageData(depth, page.title(), page.body().toString()));
				depth++;
				Elements linksOnUrl = page.select("a[href]");
				
				for(Element link : linksOnUrl)
				{
					crawl(link.attr("abs:href"), depth);
				}
				
			} 
			catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
		}
	}
	public static void main(String[] args) {
		crawl("https://www.javatpoint.com/digital-electronics", 0);
		for(String key : urlLinks.keySet())
		{
			System.out.println("Depth:"+urlLinks.get(key).depth +" Title: "+urlLinks.get(key).title);
		}
		System.out.println();	
	}

}
